# Data Scientist

#### Technical Skills: Python, Pytorch, TensorFlow, Github

## Education
- M.S., Computer Science	| The University of Quebec at Chicoutimi (_January 2025_)	 			        		
- B.S., Science | ESISAR Grenoble INP (May 2022)

## Work Experience
**Intern Software Developer @ NSE Soyons (_January 2023 - June 2023_)**
- Developed Python and TestStand software tool for efficient equipment testing.
- Enabled seamless functionality across various equipment types (e.g., power generators, oscilloscopes)
through SCPI communication integration, ensuring versatility.
- Improved testing efficiency and reliability of reports for factory agents.

**Data Scientist Intern @ NESTLE CWAR Fabrique d'Abidjan (_June 2022 - August 2022_)**
- Developed interactive Power BI report for wastewater treatment sector, enabling real-time data analysis
and trend tracking.
- Enhanced departmental monitoring for factory management.
- Designed tool for scalability across factory departments.
  
## Recent Projects

### Movie Revenue Prediction
![Box Office Prediction](/assets/img/box.png)

[Github Repository](https://github.com/PaulEm6/Movie-Revenue-Estimation)

This project aimed to predict movie revenue using machine learning, leveraging features like genre, keywords, and release date from TMDB. The main challenge was cleaning diverse data types. The pipeline included data cleaning, feature engineering, model training, evaluation, and deployment. Future enhancements may involve advanced feature engineering and model exploration. Key takeaways included navigating diverse data and understanding revenue prediction complexities.

### Image Generation
![Pokemon Generated](/assets/img/image.png)

[Github Repository](https://github.com/PaulEm6/Movie-Revenue-Estimation)

This project delves into image generation using a Deep Convolutional Generative Adversarial Network (DCGAN) applied to the Pokemon dataset. Leveraging PyTorch, I enhanced a Kaggle-based source code, experimenting with neural network architectures to refine image quality. I also optimized the code for reproducibility by implementing object-oriented design and fixing a random seed. Future endeavors entail advancing image quality through exploration of cutting-edge techniques and architectures proposed in research papers for neural network-based image generation.

### Pay Attention

![Transformer Architecture from research paper](/assets/img/attention.png)

[Github Repository](https://github.com/PaulEm6/Attention-is-all-you-need)

This project delves into the implementation of a Transformer architecture for text generation, drawing inspiration from the seminal "Attention is All You Need" paper and leveraging the PyTorch library. Despite encountering challenges in understanding both the theoretical concepts and the intricacies of the provided source code, we successfully developed a decoder-only Transformer with self-attention. Building upon the original implementation, we extended the functionality by implementing a word-based tokenization approach, exploring cross-attention mechanisms, and conducting extensive experiments with diverse training datasets, including research papers, novel chapters, and song lyrics. These efforts not only deepened our understanding of attention mechanisms and Transformer architectures but also provided valuable insights into their application in natural language processing tasks.

